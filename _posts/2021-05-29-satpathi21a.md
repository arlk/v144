---
title: The Dynamics of Gradient Descent for Overparametrized Neural Networks
abstract: We consider the dynamics of gradient descent (GD) in overparameterized single
  hidden layer neural networks with a squared loss function. Recently, it has been
  shown that, under some conditions, the parameter values obtained using GD achieve
  zero training error and generalize well if the initial conditions are chosen appropriately.
  Here, through a Lyapunov analysis, we show that the dynamics of neural network weights
  under GD converge to a point which is close to the minimum norm solution subject
  to the condition that there is no training error when using the linear approximation
  to the neural network.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: satpathi21a
month: 0
tex_title: The Dynamics of Gradient Descent for Overparametrized Neural Networks
firstpage: 373
lastpage: 384
page: 373-384
order: 373
cycles: false
bibtex_author: Satpathi, Siddhartha and Srikant, R
author:
- given: Siddhartha
  family: Satpathi
- given: R
  family: Srikant
date: 2021-05-29
address:
container-title: Proceedings of the 3rd Conference on Learning for Dynamics and Control
volume: '144'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 5
  - 29
pdf: http://proceedings.mlr.press/v144/satpathi21a/satpathi21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
