---
title: Faster Policy Learning with Continuous-Time Gradients
abstract: We study the estimation of policy gradients for continuous-time systems
  with known dynamics. By reframing policy learning in continuous-time, we show that
  it is possible construct a more efficient and accurate gradient estimator. The standard
  back-propagation through time estimator (BPTT) computes exact gradients for a crude
  discretization of the continuous-time system. In contrast, we approximate continuous-time
  gradients in the original system. With the explicit goal of estimating continuous-time
  gradients, we are able to discretize adaptively and construct a more efficient policy
  gradient estimator which we call the Continuous-Time Policy Gradient (CTPG). We
  show that replacing BPTT policy gradients with more efficient CTPG estimates results
  in faster and more robust learning in a variety of control tasks and simulators.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ainsworth21a
month: 0
tex_title: Faster Policy Learning with Continuous-Time Gradients
firstpage: 1054
lastpage: 1067
page: 1054-1067
order: 1054
cycles: false
bibtex_author: Ainsworth, Samuel and Lowrey, Kendall and Thickstun, John and Harchaoui,
  Zaid and Srinivasa, Siddhartha
author:
- given: Samuel
  family: Ainsworth
- given: Kendall
  family: Lowrey
- given: John
  family: Thickstun
- given: Zaid
  family: Harchaoui
- given: Siddhartha
  family: Srinivasa
date: 2021-05-29
address:
container-title: Proceedings of the 3rd Conference on Learning for Dynamics and Control
volume: '144'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 5
  - 29
pdf: http://proceedings.mlr.press/v144/ainsworth21a/ainsworth21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
