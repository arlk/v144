---
title: Exploiting Sparsity for Neural Network Verification
abstract: The problem of verifying the properties of a neural network has never been
  more important. This task is often done by bounding the activation functions in
  the network. Some approaches are more conservative than others and in general there
  is a trade-off between complexity and conservativeness. There has been significant
  progress to improve the efficiency and the accuracy of these methods. We investigate
  the sparsity that arises in a recently proposed semi-definite programming framework
  to verify a fully connected feed-forward neural network. We show that due to the
  intrinsic cascading structure of the neural network the constraint matrices in the
  semi-definite program form a block-arrow pattern and satisfy conditions for chordal
  sparsity. We reformulate and implement the optimisation problem, showing a significant
  speed-up in computation, without sacrificing solution accuracy.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: newton21a
month: 0
tex_title: Exploiting Sparsity for Neural Network Verification
firstpage: 715
lastpage: 727
page: 715-727
order: 715
cycles: false
bibtex_author: Newton, Matthew and Papachristodoulou, Antonis
author:
- given: Matthew
  family: Newton
- given: Antonis
  family: Papachristodoulou
date: 2021-05-29
address:
container-title: Proceedings of the 3rd Conference on Learning for Dynamics and Control
volume: '144'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 5
  - 29
pdf: http://proceedings.mlr.press/v144/newton21a/newton21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
