---
title: A unified framework for Hamiltonian deep neural networks
abstract: Training deep neural networks (DNNs) can be difficult due to the occurrence
  of vanishing/exploding gradients during weight optimization. To avoid this problem,
  we propose a class of DNNs stemming from the time discretization of Hamiltonian
  systems. The time-invariant version of Hamiltonian models enjoys marginal stability,
  a property that, as shown in previous studies, can eliminate convergence to zero
  or divergence of gradients. In the present paper, we formally show this feature
  by deriving and analysing the backward gradient dynamics in continuous time. The
  proposed Hamiltonian framework, besides encompassing existing networks inspired
  by marginally stable ODEs, allows one to derive new and more expressive architectures.
  The good performance of the novel DNNs is demonstrated on benchmark classification
  problems, including digit recognition using the MNIST dataset.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: galimberti21a
month: 0
tex_title: A unified framework for Hamiltonian deep neural networks
firstpage: 275
lastpage: 286
page: 275-286
order: 275
cycles: false
bibtex_author: Galimberti, Clara Luc\'{i}a and Xu, Liang and Trecate, Giancarlo Ferrari
author:
- given: Clara Luc√≠a
  family: Galimberti
- given: Liang
  family: Xu
- given: Giancarlo Ferrari
  family: Trecate
date: 2021-05-29
address:
container-title: Proceedings of the 3rd Conference on Learning for Dynamics and Control
volume: '144'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 5
  - 29
pdf: http://proceedings.mlr.press/v144/galimberti21a/galimberti21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
